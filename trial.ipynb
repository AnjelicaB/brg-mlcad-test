{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  -l --list-models    List supported models\n",
    "#     --model          LLM model to use (default: openai/gpt-4)\n",
    "#     --temperature    LLM model temperature (default: 0.8)\n",
    "#     --top-p          LLM model top_p (default: 0.95)\n",
    "#     --max-tokens     LLM model max_tokens (default: 1024)\n",
    "#     --examples       Number of in-context examples to use (default: 0)\n",
    "#     --lang           What language to generate (default: verilog)\n",
    "#     --output         File to write extracted code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import getpass\n",
    "import keys\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_community.chat_models import ChatAnyscale\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain_community.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]= keys.LANGCHAIN_API_KEY\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=\"pr-stupendous-reveal-16\"\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = keys.OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 11, 'total_tokens': 20, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-06a5b79a-cca9-49f4-b802-4f13c820083d-0', usage_metadata={'input_tokens': 11, 'output_tokens': 9, 'total_tokens': 20, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI()\n",
    "llm.invoke(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "# Models\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "openai_chat_models = [\n",
    "  \"gpt-4\",\n",
    "]\n",
    "\n",
    "nim_chat_models = [\n",
    "]\n",
    "\n",
    "anyscale_chat_models = [\n",
    "]\n",
    "\n",
    "all_chat_models = [\n",
    "  *openai_chat_models,\n",
    "  *nim_chat_models,\n",
    "  *anyscale_chat_models,\n",
    "]\n",
    "\n",
    "model_aliases = {\n",
    "  \"openai/gpt4\"            : \"gpt-4\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "# Context\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "verilog_system_msg=\"\"\"\n",
    "You are a Verilog RTL designer that only writes code using correct\n",
    "Verilog syntax.\n",
    "\"\"\"\n",
    "\n",
    "pymtl_system_msg=\"\"\"\n",
    "You are a PyMTL3 RTL designer that only writes code using correct\n",
    "PyMTL3 syntax.\n",
    "\"\"\"\n",
    "\n",
    "pyrtl_system_msg=\"\"\"\n",
    "You are a PyRTL RTL designer that only writes code using correct\n",
    "PyRTL syntax.\n",
    "\"\"\"\n",
    "\n",
    "myhdl_system_msg=\"\"\"\n",
    "You are a MyHDL RTL designer that only writes code using correct\n",
    "MyHDL syntax.\n",
    "\"\"\"\n",
    "\n",
    "migen_system_msg=\"\"\"\n",
    "You are a Migen RTL designer that only writes code using correct\n",
    "Migen syntax.\n",
    "\"\"\"\n",
    "\n",
    "amaranth_system_msg=\"\"\"\n",
    "You are a Amaranth RTL designer that only writes code using correct\n",
    "Amaranth syntax.\n",
    "\"\"\"\n",
    "\n",
    "prompt_no_explain_suffix=\"\"\"\n",
    "Enclose your code with <CODE> and </CODE>. Only output the code snippet\n",
    "and do NOT output anything else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =           \"gpt-4\"\n",
    "problem =         \"Prob01p01_comb_const_zero\"\n",
    "temperature =     0.8\n",
    "top_p =           0.95\n",
    "max_tokens =      1024\n",
    "examples =        0 # the code does not load any examples\n",
    "lang =            \"verilog\"\n",
    "generate_log =    f\"output/{problem}_generate.txt\"\n",
    "output =          f\"output/{problem}_output.v\"\n",
    "prompt_filename = f\"dataset/{problem}_prompt.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  system_msg = \"\"\n",
    "  if   lang == \"verilog\":  system_msg = verilog_system_msg\n",
    "  else: \n",
    "    print(\"\")\n",
    "    print(f\"ERROR: Unknown language {lang}\")\n",
    "    print(\"\")\n",
    "    return\n",
    "  \n",
    "  problem = \"?\"\n",
    "  if prompt_filename.endswith(\"_prompt.txt\"):\n",
    "    problem = os.path.basename(prompt_filename[:-11])\n",
    "  \n",
    "  print( \"\" )\n",
    "  print( f\"problem     = {problem}\"     )\n",
    "  print( f\"model       = {model}\"       )\n",
    "  print( f\"temperature = {temperature}\" )\n",
    "  print( f\"top_p       = {top_p}\"       )\n",
    "  print( f\"max_tokens  = {max_tokens}\"  )\n",
    "\n",
    "  # Read the prompt file\n",
    "\n",
    "  with open(prompt_filename) as file:\n",
    "    prompt = file.read()\n",
    "\n",
    "  # Create full prompt (does NOT load any examples)\n",
    "\n",
    "  full_prompt = \"\"\n",
    "  full_prompt += \"\\nQuestion:\\n\"\n",
    "  full_prompt += prompt.strip() + \"\\n\"\n",
    "  full_prompt = full_prompt.rstrip() + \"\\n\" + prompt_no_explain_suffix\n",
    "  full_prompt += \"\\nAnswer:\\n\"\n",
    "\n",
    "  # Print system message and prompt\n",
    "\n",
    "  print(\"\")\n",
    "  print(\"System Message\")\n",
    "  print(\"-\"*74)\n",
    "  print(system_msg)\n",
    "\n",
    "  print(\"Prompt\")\n",
    "  print(\"-\"*74)\n",
    "  print(full_prompt.rstrip())\n",
    "\n",
    "  # Create LLM messages\n",
    "\n",
    "  msgs = [ SystemMessage(system_msg), HumanMessage(full_prompt) ]\n",
    "\n",
    "  # Query the LLM\n",
    "\n",
    "  llm = ChatOpenAI(\n",
    "    model        = model,\n",
    "    temperature  = temperature,\n",
    "    top_p        = top_p,\n",
    "    max_tokens   = max_tokens,\n",
    "  )\n",
    "\n",
    "  with get_openai_callback() as cb:\n",
    "    resp = llm.invoke(msgs)\n",
    "\n",
    "  # Display the response\n",
    "\n",
    "  print(\"\")\n",
    "  print(\"Response\")\n",
    "  print(\"-\"*74)\n",
    "\n",
    "  print(\"\")\n",
    "  print(resp.content)\n",
    "  print(\"\")\n",
    "\n",
    "  # Display statistics\n",
    "\n",
    "  print(\"Statistics\")\n",
    "  print(\"-\"*74)\n",
    "  \n",
    "  print(\"\")\n",
    "  print(f\"prompt_tokens = {cb.prompt_tokens}\")\n",
    "  print(f\"resp_tokens   = {cb.completion_tokens}\")\n",
    "  print(f\"total_tokens  = {cb.total_tokens}\")\n",
    "  print(f\"total_cost    = {cb.total_cost}\")\n",
    "  print(\"\")\n",
    "\n",
    "  # Extract code from response\n",
    "  if output != \"NONE\":\n",
    "    file = open( output, 'w' )\n",
    "\n",
    "    print( \"\" , file=file)\n",
    "\n",
    "    # Scan response for code using <CODE></CODE>\n",
    "\n",
    "    found_code_lines = []\n",
    "    found_code_start = False\n",
    "    found_code_end   = False\n",
    "\n",
    "    for line in iter(resp.content.splitlines()):\n",
    "\n",
    "      if not found_code_start:\n",
    "        if line.strip() == \"<CODE>\":\n",
    "          found_code_start = True\n",
    "        elif line.lstrip().startswith(\"<CODE>\"):\n",
    "          found_code_lines.append( line.lstrip().replace(\"<CODE>\",\"\") )\n",
    "          found_code_start = True\n",
    "\n",
    "      elif found_code_start and not found_code_end:\n",
    "        if line.strip() == \"</CODE>\":\n",
    "          found_code_end = True\n",
    "        elif line.rstrip().endswith(\"</CODE>\"):\n",
    "          found_code_lines.append( line.rstrip().replace(\"</CODE>\",\"\") )\n",
    "          found_code_end = True\n",
    "        else:\n",
    "          found_code_lines.append( line )\n",
    "\n",
    "    if found_code_start and found_code_end:\n",
    "      for line in found_code_lines:\n",
    "        print( line , file=file )\n",
    "\n",
    "    # If did not find code by looking for <CODE></CODE>, then scan response\n",
    "    # for code using backticks\n",
    "\n",
    "    if not found_code_start and not found_code_end:\n",
    "\n",
    "      found_code_lines = []\n",
    "      found_code_start = False\n",
    "      found_code_end   = False\n",
    "\n",
    "      for line in iter(resp.content.splitlines()):\n",
    "\n",
    "        if not found_code_start:\n",
    "          if line.lstrip().startswith(\"```\"):\n",
    "            found_code_start = True\n",
    "\n",
    "        elif found_code_start and not found_code_end:\n",
    "          if line.rstrip().endswith(\"```\"):\n",
    "            found_code_end = True\n",
    "          else:\n",
    "            found_code_lines.append( line )\n",
    "\n",
    "      if found_code_start and found_code_end:\n",
    "        for line in found_code_lines:\n",
    "          print(line , file=file )\n",
    "\n",
    "        # Print comment so we can track responses that did not use\n",
    "        # <CODE></CODE> correctly\n",
    "\n",
    "        print( \"\" , file=file )\n",
    "        if lang == \"verilog\":\n",
    "          comment_delim = \"//\"\n",
    "        else:\n",
    "          comment_delim = \"#\"\n",
    "        print( comment_delim + \" PYHDL-EVAL: response did not use <CODE></CODE> correctly\" , file=file )\n",
    "\n",
    "    print( \"\" , file=file )\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open( generate_log, 'w' )\n",
    "orig_stdout = sys.stdout\n",
    "sys.stdout = f\n",
    "\n",
    "main()\n",
    "\n",
    "sys.stdout = orig_stdout\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlcad-local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
